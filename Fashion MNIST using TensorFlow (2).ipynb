{"cells": [{"metadata": {"scrolled": true}, "cell_type": "code", "source": "! pip install tensorflow==1.15", "execution_count": null, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting tensorflow==1.15\n  Using cached tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.12.1)\nRequirement already satisfied: tensorflow-estimator==1.15.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (0.9.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\nRequirement already satisfied: keras-applications>=1.0.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.27.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (3.1.0)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (3.12.3)\nRequirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.18.5)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\nRequirement already satisfied: gast==0.2.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (0.34.2)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (47.3.1.post20200622)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\nInstalling collected packages: tensorflow\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### \uc8fc\uc758! \uc774 \uacf3\uc5d0\uc11c \ubc18\ub4dc\uc2dc \ucee4\ub110\uc744 restart\uc2dc\ud0a4\uc2ed\uc2dc\uc624."}, {"metadata": {}, "cell_type": "code", "source": "# Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\n%matplotlib inline\nimport os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "print(tf.__version__)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data = input_data.read_data_sets('data/fashion',one_hot=True, source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "! ls -l data/fashion", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Shapes of training set\nprint(\"Training set (images) shape: {shape}\".format(shape=data.train.images.shape))\nprint(\"Training set (labels) shape: {shape}\".format(shape=data.train.labels.shape))\n\n# Shapes of test set\nprint(\"Test set (images) shape: {shape}\".format(shape=data.test.images.shape))\nprint(\"Test set (labels) shape: {shape}\".format(shape=data.test.labels.shape))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Create dictionary of target classes\nlabel_dict = {\n 0: 'T-shirt/top',\n 1: 'Trouser',\n 2: 'Pullover',\n 3: 'Dress',\n 4: 'Coat',\n 5: 'Sandal',\n 6: 'Shirt',\n 7: 'Sneaker',\n 8: 'Bag',\n 9: 'Ankle boot',\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data.test.labels", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(data.train.images[0], (28,28))\ncurr_lbl = np.argmax(data.train.labels[0,:])\nplt.imshow(curr_img, cmap='gray')\nplt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(data.test.images[0], (28,28))\ncurr_lbl = np.argmax(data.test.labels[0,:])\nplt.imshow(curr_img, cmap='gray')\nplt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data.train.images[0][500:]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data.train.labels", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(np.max(data.train.images[0]))\nprint(np.min(data.train.images[0]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Reshape training and testing image\ntrain_X = data.train.images.reshape(-1, 28, 28, 1)\ntest_X = data.test.images.reshape(-1,28,28,1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_X.shape, test_X.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_y =data.train.labels\ntest_y = data.test.labels", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_y.shape, test_y.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815\ntraining_iters = 5\nlearning_rate = 0.001\nbatch_size = 128", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# MNIST data input (img shape: 28*28)\nn_input = 28\n\n# MNIST total classes (0-9 digits)\nn_classes = 10", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#both placeholders are of type float\nx = tf.placeholder(\"float\", [None, 28,28,1])\ny = tf.placeholder(\"float\", [None, n_classes])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def conv2d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.sigmoid(x)\n\ndef maxpool2d(x, k=2):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "weights = {\n    'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()),\n    'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()),\n    'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()),\n    'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()),\n    'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()),\n}\nbiases = {\n    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n    'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n    'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def conv_net(x, weights, biases):  \n\n    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Convolution Layer\n    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n    conv2 = maxpool2d(conv2, k=2)\n\n    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n    conv3 = maxpool2d(conv3, k=2)\n\n\n    # Fully connected layer\n    # Reshape conv2 output to fit fully connected layer input\n    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.sigmoid(fc1)\n    # Output, class prediction\n    # finally we multiply the fully connected layer with the weights and add a bias term.\n    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n    return out", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pred = conv_net(x, weights, biases)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Here, you check whether the index of the maximum value of the predicted image is equal to the actual labeled image. And both will be a column vector.\ncorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n\n#calculate accuracy across all the given images and average them out.\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Initializing the variables\ninit = tf.global_variables_initializer()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "with tf.Session() as sess:\n    sess.run(init)\n    train_loss = []\n    test_loss = []\n    train_accuracy = []\n    test_accuracy = []\n    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n    for i in range(training_iters):\n        for batch in range(len(train_X)//batch_size):\n            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n            # Run optimization op (backprop).\n                # Calculate batch loss and accuracy\n            opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n        print(\"Iter \" + str(i) + \", Loss= \" + \\\n                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                      \"{:.5f}\".format(acc))\n        print(\"Optimization Finished!\")\n\n        # Calculate accuracy for all 10000 mnist test images\n        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n        train_loss.append(loss)\n        test_loss.append(valid_loss)\n        train_accuracy.append(acc)\n        test_accuracy.append(test_acc)\n        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n    summary_writer.close()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\nplt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\nplt.title('Training and Test loss')\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.legend()\nplt.figure()\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')\nplt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')\nplt.title('Training and Test Accuracy')\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.legend()\nplt.figure()\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')\nplt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\nplt.title('Training Accuracy vs. Test Loss')\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.legend()\nplt.figure()\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}