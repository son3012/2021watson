{"cells": [{"metadata": {"scrolled": true}, "cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\n\nif os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n    endpoint_898a030403f7485eb967ba42b0aa73e9 = 'https://s3-api.us-geo.objectstorage.softlayer.net'\nelse:\n    endpoint_898a030403f7485eb967ba42b0aa73e9 = 'https://s3-api.us-geo.objectstorage.service.networklayer.com'\n\nclient_898a030403f7485eb967ba42b0aa73e9 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='80KirTCmV-TTHxpLeuJCdJZoKY7XXZniiMc5BuQKu19L',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url=endpoint_898a030403f7485eb967ba42b0aa73e9)\n\nbody = client_898a030403f7485eb967ba42b0aa73e9.get_object(Bucket='p041pandas-donotdelete-pr-q1bjzjv05rkhwl',Key='bankloanData.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_1 = pd.read_csv(body)\ndf_data_1.head()\n", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "   age  ed  employ  address  income  debtinc   creddebt   othdebt default  \\\n0   41   3      17       12     176      9.3  11.359392  5.008608       1   \n1   27   1      10        6      31     17.3   1.362202  4.000798       0   \n2   40   1      15       14      55      5.5   0.856075  2.168925       0   \n3   41   1      15       14     120      2.9   2.658720  0.821280       0   \n4   24   2       2        0      28     17.3   1.787436  3.056564       1   \n\n   preddef1  preddef2  preddef3  \n0  0.808394  0.788640  0.213043  \n1  0.198297  0.128445  0.436903  \n2  0.010036  0.002987  0.141023  \n3  0.022138  0.010273  0.104422  \n4  0.781588  0.737885  0.436903  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>ed</th>\n      <th>employ</th>\n      <th>address</th>\n      <th>income</th>\n      <th>debtinc</th>\n      <th>creddebt</th>\n      <th>othdebt</th>\n      <th>default</th>\n      <th>preddef1</th>\n      <th>preddef2</th>\n      <th>preddef3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>3</td>\n      <td>17</td>\n      <td>12</td>\n      <td>176</td>\n      <td>9.3</td>\n      <td>11.359392</td>\n      <td>5.008608</td>\n      <td>1</td>\n      <td>0.808394</td>\n      <td>0.788640</td>\n      <td>0.213043</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27</td>\n      <td>1</td>\n      <td>10</td>\n      <td>6</td>\n      <td>31</td>\n      <td>17.3</td>\n      <td>1.362202</td>\n      <td>4.000798</td>\n      <td>0</td>\n      <td>0.198297</td>\n      <td>0.128445</td>\n      <td>0.436903</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40</td>\n      <td>1</td>\n      <td>15</td>\n      <td>14</td>\n      <td>55</td>\n      <td>5.5</td>\n      <td>0.856075</td>\n      <td>2.168925</td>\n      <td>0</td>\n      <td>0.010036</td>\n      <td>0.002987</td>\n      <td>0.141023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>41</td>\n      <td>1</td>\n      <td>15</td>\n      <td>14</td>\n      <td>120</td>\n      <td>2.9</td>\n      <td>2.658720</td>\n      <td>0.821280</td>\n      <td>0</td>\n      <td>0.022138</td>\n      <td>0.010273</td>\n      <td>0.104422</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>28</td>\n      <td>17.3</td>\n      <td>1.787436</td>\n      <td>3.056564</td>\n      <td>1</td>\n      <td>0.781588</td>\n      <td>0.737885</td>\n      <td>0.436903</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.dtypes", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "age           int64\ned            int64\nemploy        int64\naddress       int64\nincome        int64\ndebtinc     float64\ncreddebt    float64\nothdebt     float64\ndefault      object\npreddef1    float64\npreddef2    float64\npreddef3    float64\ndtype: object"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.values", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "array([[41, 3, 17, ..., 0.808394327359702, 0.7886404318214371,\n        0.21304337612811897],\n       [27, 1, 10, ..., 0.19829747615910395, 0.128445387038174,\n        0.43690300550604605],\n       [40, 1, 15, ..., 0.0100361080990023, 0.00298677834821412,\n        0.141022623460993],\n       ...,\n       [48, 1, 13, ..., 0.0301374981044824, 0.0325702625943738,\n        0.24801041775523303],\n       [35, 2, 1, ..., 0.26900345101699397, 0.37854649636973203,\n        0.181814378077261],\n       [37, 1, 20, ..., 0.006397812918809229, 0.0111731232851226,\n        0.30304155578236497]], dtype=object)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\n\n# Make a numpy array from the dataframe, except remove rows with no value for 'default'\ni = list(df_data_1.columns.values).index('default')\ndata = np.array([x for x in df_data_1.values if x[i] in ['0', '1']])\n\n# Remove the columns for preddef1, predef2 and preddef3\ndata = np.delete(data, slice(9,12), axis=1)\n\n# Separate the 'predictors' (aka 'features') from the dependent variable (aka 'label') \n# that we will learn how to predict\npredictors = np.delete(data, 8, axis=1)\ndependent = np.delete(data, slice(0, 8), axis=1)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Convert the label type to numeric categorical representing the classes to predict (binary classfier)\ndependent = dependent.astype(int)\n\n# And flatten it to one dimensional for use as the expected output label vector in TensorFlow\ndependent = dependent.flatten()\ndependent\n\n# Convert all the predictors to float to simplify this demo TensorFlow code\npredictors = predictors.astype(float)\n\n# Get the shape of the predictors\nm, n = predictors.shape\nm, n", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "(700, 8)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "predictors[0]", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "array([ 41.      ,   3.      ,  17.      ,  12.      , 176.      ,\n         9.3     ,  11.359392,   5.008608])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "dependent", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# Partition the input data into a training set and a test set\n\nm_train = 500\nm_test = m - m_train\n\npredictors_train = predictors[:m_train]\ndependent_train = dependent[:m_train]\n\npredictors_test = predictors[m_train:]\ndependent_test = dependent[m_train:]\n\n# Gets a batch of the training data. \n# NOTE: Rather than loading a whole large data set as above and then taking array slices as done here, \n#       This method can connect to a data source and select just the batch needed.\ndef get_training_batch(batch_num, batch_size):\n    lower = batch_num * (m_train // batch_size)\n    upper = lower + batch_size\n    return predictors_train[lower:upper], dependent_train[lower:upper]", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import tensorflow as tf\n\n# Make this notebook's output stable across runs\ntf.reset_default_graph()\ntf.set_random_seed(42)\nnp.random.seed(42)\n\n# A method to build a new neural net layer of a given size,  \n# fully connect it to a given preceding layer X, and \n# compute its output Z either with or without (default) an activation function\n# Call with activation=tf.nn.relu or tf.nn.sigmoid or tf.nn.tanh, for examples\n\ndef make_nn_layer(layer_name, layer_size, X, activation=None):\n    with tf.name_scope(layer_name):\n        X_size = int(X.get_shape()[1])\n        SD = 2 / np.sqrt(X_size)\n        weights = tf.truncated_normal((X_size, layer_size), dtype=tf.float64, stddev=SD)\n        W = tf.Variable(weights, name='weights')\n        b = tf.Variable(tf.zeros([layer_size], dtype=tf.float64), name='biases')\n        Z = tf.matmul(X, W) + b\n        if activation is not None:\n            return activation(Z)\n        else:\n            return Z", "execution_count": 13, "outputs": [{"output_type": "error", "ename": "AttributeError", "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m<ipython-input-13-01f6988c4008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make this notebook's output stable across runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"]}]}, {"metadata": {}, "cell_type": "code", "source": "# Make the neural net structure\n\nn_inputs = n\nn_hidden1 = n \n### n_hidden2 = n // 2\nn_outputs = 2   # Two output classes: defaulting or non-defaulting on loan\n\nX = tf.placeholder(tf.float64, shape=(None, n_inputs), name='X')\n\nwith tf.name_scope('nn'):\n    hidden1 = make_nn_layer('hidden1', n_hidden1, X, activation=tf.nn.relu)\n    hidden2 = hidden1\n    ### hidden2 = make_nn_layer('hidden2', n_hidden2, hidden1, activation=tf.nn.relu)\n    outputs = make_nn_layer('outputs', n_outputs, hidden2) \n    outputs = tf.identity(outputs, \"nn_output\")\n    \ny = tf.placeholder(tf.int64, shape=(None), name='y')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define how the neural net will learn\n\nwith tf.name_scope('loss'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n    loss = tf.reduce_mean(xentropy, name='l')\n    \nlearning_rate = 0.01\nwith tf.name_scope(\"train\"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n    \nwith tf.name_scope(\"test\"):\n    correct = tf.nn.in_top_k(tf.cast(outputs, tf.float32), y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Set up the ability to save and restore the trained neural net...\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# ... and make a subdirectory space in which to save the model files (only need to run this once)\n!mkdir \"../datasets\"\n!mkdir \"../datasets/Neural Net\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# TRAINING TIME\n\n# This is how many times to use the full set of training data\nn_epochs = 3000\n\n# For a larger training set, it's typically necessary to break training into\n# batches so only the memory needed to store one batch of training data is used\nbatch_size = 50\n\nwith tf.Session() as training_session:\n    init.run()\n    \n    for epoch in range(n_epochs):\n        \n        # Shuffling (across batches) is easier to do for small data sets and\n        # helps increase accuracy\n        training_set = [[pt_elem, dependent_train[i]] for i, pt_elem in enumerate(predictors_train)]\n        np.random.shuffle(training_set)\n        predictors_train = [ts_elem[0] for ts_elem in training_set]\n        dependent_train = [ts_elem[1] for ts_elem in training_set]\n        \n        # Loop through the whole training set in batches\n        for batch_num in range(m_train // batch_size):\n            X_batch, y_batch = get_training_batch(batch_num, batch_size)\n            training_session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n\n        if epoch % 100 == 99:\n            acc_train = accuracy.eval(feed_dict={X: predictors_train, y: dependent_train})\n            acc_test = accuracy.eval(feed_dict={X: predictors_test, y: dependent_test})\n            print(epoch+1, \"Training accuracy:\", acc_train, \"Testing accuracy:\", acc_test)\n\n    save_path = saver.save(training_session, \"../datasets/Neural Net/Neural Net.ckpt\")\n    \n    # A quick test with the trained model \n    Z = outputs.eval(feed_dict={X: predictors_test[:20]})\n    dependent_pred = np.argmax(Z, axis=1)\n    print(\"\")\n    print(\"Actual classes:   \", dependent_test[:20])  \n    print(\"Predicted classes:\", dependent_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Restore the saved model and use it to perform inference on a \"received\" new set of data\n\n# We will simulate \"receiving\" the new data by taking a slice of the test set.\n##predictors_received = predictors_test[20:40]\npredictors_received = predictors_test[:]\n\nimport tensorflow as tf_inference\n\nwith tf_inference.Session() as inference_session:\n    inf_saver = tf_inference.train.import_meta_graph('../datasets/Neural Net/Neural Net.ckpt.meta')\n    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../datasets/Neural Net/'))\n    \n    graph = tf_inference.get_default_graph()\n    X = graph.get_tensor_by_name(\"X:0\")\n    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n\n    Z = inference_session.run(nn_output, feed_dict={X: predictors_received})\n    dependent_pred = np.argmax(Z, axis=1)\n    \n    dependent_prob = inference_session.run(tf_inference.nn.softmax(nn_output), feed_dict={X: predictors_received})\n\n    confidences = [p[dependent_pred[i]] for i, p in enumerate(dependent_prob)]\n    \nprint(\"Actual classes:   \", dependent_test[20:40])\n## print(\"Predicted classes:\", dependent_pred)\nprint(\"Predicted classes:\", dependent_pred[20:40])\nprint(\"\")\n## print(\"Confidences: \", confidences)\nprint(\"Confidences: \", confidences[20:40])\nprint(\"\")\n## print(\"Probabilities: \", dependent_prob)\nprint(\"Probabilities: \", dependent_prob[20:40])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# List the model files\n!ls -l \"../datasets/Neural Net\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Now we're going to assess the quality of the neural net using ROC curve and AUC\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# send the actual dependent variable classifications for param 1, \n# and the confidences of the true classification for param 2.\nFPR, TPR, _ = roc_curve(dependent_test, dependent_prob[:, 1])\n\n# Calculate the area under the confidence ROC curve.\n# This area is equated with the probability that the classifier will rank \n# a randomly selected defaulter higher than a randomly selected non-defaulter.\nAUC = auc(FPR, TPR)\n\n# What is \"good\" can dependm but an AUC of 0.7+ is generally regarded as good, \n# and 0.8+ is generally regarded as being excellent \nAUC", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Now we'll plot the confidence ROC curve \nplt.figure()\nplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % AUC)\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.02])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import classification_report\nprint(classification_report(dependent_test, dependent_pred))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Say you want a model that is very accurate at recalling true positives (defaulters), \n# even if it gets a lot a false positives (non-defaulters). You might be automatically\n# excepting the false classifications and, for true classifications, you may send them\n# for human review rather than rejecting their loan applications.\n\n# The lowest confidence that can give 100% TPR on the test set is equal to the \n# true class with the lowest confidence, so we'll find that now\ndefaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 1]\nmin_conf = np.min(defaulter_probs)\nmin_conf", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Each non-defaulter with a confidence at or above min_conf would be predicted \n# to be a defaulter (which would be a false positve prediction for a non-defaulter)\n\nnon_defaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 0]\nfalse_positives = [x for x in non_defaulter_probs if x >= min_conf]\n\ntotal = len(defaulter_probs) + len(non_defaulter_probs)\ntotal_correct = total - len(false_positives)\naccuracy = float(total_correct) / total\n\n# Overall accuracy would suffer quite a bit, but this achieves \n# the desired high accuracy on true positive identification (defaulters)  \naccuracy", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## For when you want to wipe out the training and do it again\n# !rm -rf \"../datasets/Neural Net\"\n## !rm -rf \"../datasets\"", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}