{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_YHvLkqjCrc"
   },
   "source": [
    "#IBM 왓슨 인공지능 과정 기말고사\n",
    "### 일자 : 2021년 6월 9일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb_TX0mojY_5"
   },
   "source": [
    "##이름 : '응웬득박선'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIe4C3EfkTDV"
   },
   "source": [
    "#문제 1 : Gensim을 이용한 Word2Vec(15점)\n",
    "### 다음 문제는 gensim API를 사용하여 Word2Vec으로 word embedding 모델을 구현하는 문제입니다. 여러분이 실습하였던 네이버 영화평 과제와 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ypnrK__LrPUd"
   },
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLkGIxkmTB3R"
   },
   "source": [
    "word embedding 모델을 만들 데이터셋을 다운로드받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIdqVu1KmRNr",
    "outputId": "dd6d17c3-51e7-477d-ab84-2214a51f0d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "! wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9\" -O reviews_data.txt.gz && rm -rf /tmp/cookies.txt\n",
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T6Z-46STIQV"
   },
   "source": [
    "다운로드 받은 파일을 한 줄 씩 읽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVe9Mgutmda-"
   },
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkoZ1y6eNQlp"
   },
   "source": [
    "##문제1-1. Word2Vec API를 사용하여 다운로드받은 파일에 대한 word embedding 모델을 트레이닝시키십시오.(5점)\n",
    "Word2Vec의 파라미터는 다음과 같이 적용하십시오. \n",
    "size=150, window=10, min_count=2, workers=10, iter=1\n",
    "\n",
    "트레이닝 시키는데 2분 가량 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i55O0qECmiLm"
   },
   "outputs": [],
   "source": [
    "%time model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMiqYJmLnsJw"
   },
   "source": [
    "##문제1-2. gensim의 API를 사용하여 'dirty'의 동의어들을 찾아내십시오. \n",
    "아래 링크의 네이버 영화평 코드를 참조하십시오.\n",
    "\n",
    "참조 : https://colab.research.google.com/drive/1kY7mpCXo_5RDB-WpTO_4bARDFaZB_Vyi#scrollTo=45SmN7RK7hJb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1Lne2vO8mjfn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting konlpy\n",
      "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from konlpy) (1.18.5)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from konlpy) (4.6.3)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 48.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Installing collected packages: beautifulsoup4, JPype1, colorama, tweepy, konlpy\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.1\n",
      "    Uninstalling beautifulsoup4-4.9.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.1\n",
      "  Attempting uninstall: JPype1\n",
      "    Found existing installation: JPype1 0.6.3\n",
      "    Uninstalling JPype1-0.6.3:\n",
      "      Successfully uninstalled JPype1-0.6.3\n",
      "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n",
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from gensim) (1.18.5)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.1 smart-open-5.1.0\n"
     ]
    }
   ],
   "source": [
    "# dirty와 가장 similar한 단어들을 출력하십시오.\n",
    "! pip  install konlpy\n",
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV48dmBwQik4"
   },
   "source": [
    "##문제1-3. gensim의 API를 사용하여 'dirty'와 'unclean'간의 코사인 유사도(cosine similarity)를 출력하십시오.\n",
    "다음 링크의 similarity 계산 예제를 참고하십시오.\n",
    "\n",
    "참고 https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U-RaLt7msul",
    "outputId": "a47df975-031c-471a-cfbe-9d2d36d38d21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8418293"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between 'dirty' and 'unclean'\n",
    "your_code_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU9JKdNvrP2w"
   },
   "source": [
    "#문제 2: RNN을 이용한 한국어 문장 생성기 코딩(15점)\n",
    "### 다음 코드 중 your_code_here 부분을 완성하십시오. P10-1의 \"글쓰는 인공지능2 실습\"에 사용된 코드를 참고하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZDuqyL_rmYG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkcsdQrnVAY2"
   },
   "source": [
    "학습할 입력 문장을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32jTHO15VAzN"
   },
   "outputs": [],
   "source": [
    "text=\"\"\"특히 최근 수도권에서는 교회 소모임 참석자에 이어 이들 가족과 지인으로 번지는 2차 감염 사례가 증가하고 있습니다. 중앙재난안전대책본부는 오늘까지 수도권 교회와 관련한 코로나19 확진자는 총 63명이라고 밝혔으며  2차 감염자는 33명 이라고 전했습니다. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9R3xTkxT1S4"
   },
   "source": [
    "데이터 전처리를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTQUpKieSpdw"
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)\n",
    "\n",
    "\n",
    "print(t.word_index)\n",
    "\n",
    "sequences = list()\n",
    "for line in text.split('\\n'): \n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
    "\n",
    "\n",
    "max_len=max(len(l) for l in sequences) \n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A94dnqA8T8qg"
   },
   "source": [
    "Word Embedding 모델을 Keras로 정의하고 트레이닝을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3xzFR9gTpGE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))  \n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=120, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is_TW5rxUSla"
   },
   "source": [
    "##문제2-1~3. 이 셀에서 3 곳의 your_code_here를 찾아서 코드를 완성하십시오.(15점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLQpmYxSTqN-"
   },
   "outputs": [],
   "source": [
    "# 이 블록에서 yoou_code_here 부분을 완성하시오\n",
    "def sentence_generation(model, t, current_word, n): # model = 모델, t = 토크나이저, current_word = 현재 단어, n = 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
    "        result = np.argmax(model.predict(encoded), axis=-1)\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어의 인덱스 값이 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        # 아래 부분의 코딩을 완성하십시오        \n",
    "        current_word = your_code_here # 현재 단어 + ' ' + 예측한 단어를 current_word로 저장(5점)\n",
    "        sentence = your_code_here  # 예측한 단어를 sentence에 append(5점)\n",
    "     \n",
    "    sentence = your_code_here # 초기의 텍스트와 for loop에서 생성된 텍스트를 concatenate(5점)\n",
    "    return sentence\n",
    "\n",
    "print(sentence_generation(model, t, '특히', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXAXa6AXrqbU"
   },
   "source": [
    "#문제3 : Keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVsWQo8DVNsL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import applications\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLIiyRQ4NJ6W"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c73LL_nXQ2de"
   },
   "outputs": [],
   "source": [
    "# load CIFAR-10 dataset\n",
    "(img_train, y_train), (img_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyi1MLXnSg7q"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = img_train.astype('float32') / 255.0\n",
    "x_test = img_test.astype('float32') / 255.0\n",
    "\n",
    "img_rows, img_cols = 32,32\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEqPtul5i5mK"
   },
   "source": [
    "##문제3-1. 전이학습 : VGG16을 pre-trained 모델로  사용할 수 있도록 다음 셀의 코드를 완성하십시오.(5점)\n",
    "아래의 셀에서는 VGG16의 Pre-trained 모델을 사용합니다. 이 방법에 대하여는 P11-2 VGG16실습을 참고하십시오.\n",
    "### 참고 https://keras.io/ko/applications/#vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot6sWWoVK-Nb"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def model_builder(hp):\n",
    "    base_model = applications.VGG16(weights='imagenet', pooling='avg', include_top='False') # your_code_here에 적절한 값을 넣으십시오.\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = base_model(inputs)\n",
    "    for i in range(hp.Int('num_layers', min_value=2, max_value=5, step=1)):\n",
    "        x = Dense(units=hp.Int('units' + str(i), min_value=32, max_value=128, step=32), activation='relu')(x)\n",
    "    x = Dense(10, activation='softmax')(x) \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qLUZsnImR8p"
   },
   "source": [
    "##문제3-2. 아래 셀에 tuner 오브젝트 생성하는 코드를 완성하시오. 이 튜너는 RandomSearch를 사용하고 그를 위한 인수값은 다음을 사용하시오.(5점)\n",
    "\n",
    "\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuning_dir',\n",
    "    project_name='ktuner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_GUsErT17u1"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials=3,executions_per_trial=1,\n",
    "                    directory='tuning_dir',\n",
    "                    project_name='ktuner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2g2WQgc5tdo"
   },
   "outputs": [],
   "source": [
    "#Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n",
    "%time tuner.search(x_train, y_train, epochs = 1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlTbmaKYStFL"
   },
   "source": [
    "##문제3-3. 최적의 하이퍼 파라미터들의 조합을 print문으로 출력하시오. (5점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCjyCGlyOgEe",
    "outputId": "d82003f3-9d77-466e-e51d-3db1a8074163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 4, 'units0': 96, 'units1': 128, 'learning_rate': 0.001, 'units2': 32, 'units3': 32}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "the hyperparameter search is complete. The optimal number of units in the  first densely_connected layer is {best_hps.get('units')} and the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\") #아래에 예시한 하이퍼파라미터들을 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2_mC3_7Se80"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yqKdFCnR_kJ",
    "outputId": "b007ffff-508f-48fd-bda5-fa1896465413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 512)               14714688  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 96)                49248     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               12416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 14,781,866\n",
      "Trainable params: 14,781,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNlFFkFOijfn"
   },
   "source": [
    "#코딩 테스트를 완료하였습니다. 1학기-기말고사-yourname.ipynb로 다운로드받아서 LMS와 깃헙에 과제물로 제출해주십시오.\n",
    "#수고 하였습니다!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "watsonAI-finalTest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
